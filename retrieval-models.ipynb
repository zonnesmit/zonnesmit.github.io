{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zonnesmit/zonnesmit.github.io/blob/main/retrieval-models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUpmSFflwzR1"
      },
      "source": [
        "# Retrieval Models and Evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2PzgU4YxIl3"
      },
      "source": [
        "In this notebook, you will evaluate results ranking on a test collection. First, you'll compute the mean average precision of a baseline BM25 model. Then you'll implement a query-likelihood retrieval model based on the same inverted index.\n",
        "\n",
        "This notebook uses the [Pyserini](http://pyserini.io/) library, a Python interface to [Anserini](http://anserini.io) and thus to [Lucene](https://lucene.apache.org/), a widely-used open-source search engine. This library is written and maintained by Jimmy Lin and his colleagues at the University of Waterloo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2vNd7bpJlDZ"
      },
      "source": [
        "We start by installing the python interface. Since it calls the underlying Lucene search engine, we make sure we point to an appropriate Java installation. If you don't have Java 21, this would need to be changed."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Uncomment the following code to install Java 21 on Colab\n",
        "!apt-get install openjdk-21-jre-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-21-openjdk-amd64\"\n",
        "!update-alternatives --set java /usr/lib/jvm/java-21-openjdk-amd64/bin/java\n",
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBDwrCizYdak",
        "outputId": "798305fb-d9e8-4143-c502-f73745bae0f3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"21.0.4\" 2024-07-16\n",
            "OpenJDK Runtime Environment (build 21.0.4+7-Ubuntu-1ubuntu222.04)\n",
            "OpenJDK 64-Bit Server VM (build 21.0.4+7-Ubuntu-1ubuntu222.04, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_lt0-pXJia0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5442e346-0074-43d0-9ae9-afc0c61bc7cb"
      },
      "source": [
        "!pip install pyserini\n",
        "# You can change this to gpu if you have one.\n",
        "# It's a pyserini dependency, but we won't need it until the next assignment.\n",
        "!pip install faiss-cpu"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyserini\n",
            "  Downloading pyserini-0.40.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pyserini) (4.66.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from pyserini) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pyserini) (2.32.3)\n",
            "Requirement already satisfied: Cython>=0.29.21 in /usr/local/lib/python3.10/dist-packages (from pyserini) (3.0.11)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from pyserini) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from pyserini) (2.2.2)\n",
            "Collecting pyjnius>=1.6.0 (from pyserini)\n",
            "  Downloading pyjnius-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.10/dist-packages (from pyserini) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pyserini) (1.13.1)\n",
            "Requirement already satisfied: transformers>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from pyserini) (4.44.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyserini) (2.5.0+cu121)\n",
            "Collecting onnxruntime>=1.8.1 (from pyserini)\n",
            "  Downloading onnxruntime-1.20.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: openai>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyserini) (1.52.2)\n",
            "Requirement already satisfied: sentencepiece>=0.2 in /usr/local/lib/python3.10/dist-packages (from pyserini) (0.2.0)\n",
            "Collecting tiktoken>=0.4.0 (from pyserini)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting flask>3.0 (from pyserini)\n",
            "  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: pillow>=10.2.0 in /usr/local/lib/python3.10/dist-packages (from pyserini) (10.4.0)\n",
            "INFO: pip is looking at multiple versions of pyserini to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pyserini\n",
            "  Downloading pyserini-0.39.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting nmslib>=2.0.6 (from pyserini)\n",
            "  Downloading nmslib-2.1.1.tar.gz (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lightgbm>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from pyserini) (4.5.0)\n",
            "Requirement already satisfied: spacy>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from pyserini) (3.7.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from pyserini) (17.0.0)\n",
            "Collecting pybind11>=2.11.0 (from pyserini)\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "INFO: pip is looking at multiple versions of nmslib to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting nmslib>=2.0.6 (from pyserini)\n",
            "  Downloading nmslib-2.0.6.tar.gz (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from nmslib>=2.0.6->pyserini) (5.9.5)\n",
            "Collecting coloredlogs (from onnxruntime>=1.8.1->pyserini)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.8.1->pyserini) (24.3.25)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.8.1->pyserini) (24.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.8.1->pyserini) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.8.1->pyserini) (1.13.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->pyserini) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->pyserini) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->pyserini) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->pyserini) (0.6.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->pyserini) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->pyserini) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->pyserini) (4.12.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->pyserini) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->pyserini) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->pyserini) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.1->pyserini) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.1->pyserini) (3.5.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (0.12.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (75.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (3.4.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken>=0.4.0->pyserini) (2024.9.11)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->pyserini) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->pyserini) (0.24.7)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->pyserini) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->pyserini) (0.19.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.0.0->pyserini) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.0.0->pyserini) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.0.0->pyserini) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.0.0->pyserini) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0.0->pyserini) (0.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.6.0->pyserini) (2024.10.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.2.1->pyserini) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.0.0->pyserini) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.0.0->pyserini) (2.23.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->pyserini) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pyserini) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pyserini) (2.2.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.2.1->pyserini) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.2.1->pyserini) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.2.1->pyserini) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.2.1->pyserini) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.2.1->pyserini) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.2.1->pyserini) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.2.1->pyserini) (7.0.5)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.8.1->pyserini)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=3.2.1->pyserini) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.8.1->pyserini) (1.3.0)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.2.1->pyserini) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2.1->pyserini) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2.1->pyserini) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3.2.1->pyserini) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2.1->pyserini) (0.1.2)\n",
            "Downloading pyserini-0.39.0-py3-none-any.whl (180.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.20.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Downloading pyjnius-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: nmslib\n",
            "  Building wheel for nmslib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nmslib: filename=nmslib-2.0.6-cp310-cp310-linux_x86_64.whl size=13416754 sha256=5e5ac2a2aa17f84bedc2678eba29f1777c6223822bc0060a32f5829648bfb816\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/1d/9a/46518f2d45f6c7bd1776c699db1229de404661db2287914df3\n",
            "Successfully built nmslib\n",
            "Installing collected packages: pyjnius, pybind11, humanfriendly, tiktoken, nmslib, coloredlogs, onnxruntime, pyserini\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 nmslib-2.0.6 onnxruntime-1.20.0 pybind11-2.13.6 pyjnius-1.6.1 pyserini-0.39.0 tiktoken-0.8.0\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Downloading faiss_cpu-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkD0kKxW9mHP"
      },
      "source": [
        "You can use the `LuceneSearcher` to search over an index. We can initialize the searcher with a pre-built index, which Pyserini will automatically download if it hasn't already:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVoAZvuAI_la",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d318d31-8cf8-4469-e505-3b7bb9360acd"
      },
      "source": [
        "from pyserini.search.lucene import LuceneSearcher\n",
        "\n",
        "searcher = LuceneSearcher.from_prebuilt_index('robust04')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading index at https://rgw.cs.uwaterloo.ca/pyserini/indexes/lucene/lucene-inverted.disk45.20240803.36f7e3.tar.gz...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "lucene-inverted.disk45.20240803.36f7e3.tar.gz: 1.66GB [00:54, 32.7MB/s]                            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6xHyonHJDKy"
      },
      "source": [
        "Now we can search for a query and inspect the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFZlcqEX0t1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c3f26e2-fb84-4b8b-eecc-599c726809c7"
      },
      "source": [
        "hits = searcher.search('black bear attacks', 1000)\n",
        "\n",
        "# Prints the first 10 hits\n",
        "for i in range(0, 10):\n",
        "    print(f'{i+1:2} {hits[i].docid:15} {hits[i].score:.5f}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1 LA092790-0015   7.06680\n",
            " 2 LA081689-0039   6.89020\n",
            " 3 FBIS4-16530     6.61630\n",
            " 4 LA102589-0076   6.46450\n",
            " 5 FT932-15491     6.25090\n",
            " 6 FBIS3-12276     6.24630\n",
            " 7 LA091090-0085   6.17030\n",
            " 8 FT922-13519     6.04270\n",
            " 9 LA052790-0205   5.94060\n",
            "10 LA103089-0041   5.90650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emoSTga_7fOB"
      },
      "source": [
        "The `IndexReaderUtils` class provides various methods to read the index directly. For example, we can fetch a raw document from the index given its `docid`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5ApT1YG71mz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "outputId": "43632afb-4244-42cf-de60-3daa07bb57dd"
      },
      "source": [
        "from pyserini.index import IndexReader\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "reader = IndexReader.from_prebuilt_index('robust04')\n",
        "\n",
        "doc = reader.doc('LA092790-0015').raw()\n",
        "display(HTML('<div style=\"font-family: Times New Roman; padding-bottom:10px\">' + doc + '</div>'))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style=\"font-family: Times New Roman; padding-bottom:10px\"><DATE>\n",
              "<P>\n",
              "September 27, 1990, Thursday, Ventura County Edition\n",
              "</P>\n",
              "</DATE>\n",
              "<HEADLINE>\n",
              "<P>\n",
              "HUNGRY WILDLIFE STRAYING INTO SUBURBS;\n",
              "</P>\n",
              "<P>\n",
              "DROUGHT: FOUR DRY YEARS HAVE PARCHED NATIVE VEGETATION, FORCING BOBCATS, BEARS,\n",
              "MOUNTAIN LIONS, DEER AND COYOTES TO FORAGE CLOSER TO INHABITED AREAS.\n",
              "</P>\n",
              "</HEADLINE>\n",
              "<TEXT>\n",
              "<P>\n",
              "Hungry bobcats, bears and mountain lions -- unable to find food in Ventura\n",
              "County's drought-parched forests -- are being pushed out of their natural\n",
              "habitats to scavenge in rural communities, game officials said Wednesday.\n",
              "</P>\n",
              "<P>\n",
              "Two weeks ago, a black bear ripped the door off a trailer home in Rose Valley\n",
              "just north of Ojai. And within the past month, there have been several reports\n",
              "of mountain lions eating livestock near Los Padres National Forest. Several\n",
              "bobcats have been reported near houses in the Ojai Valley.\n",
              "</P>\n",
              "<P>\n",
              "Authorities say that over the past two years they have received twice the\n",
              "complaints -- about 20 a month -- of wild animals in populated areas. The\n",
              "drought is now in its fourth year in California.\n",
              "</P>\n",
              "<P>\n",
              "\"We've been having more and more conflicts with animals,\" said Capt. Roger\n",
              "Reese, with the state Department of Fish and Game. \"The fact is, it's very dry\n",
              "out there, and there just isn't a lot of food and water for them.\"\n",
              "</P>\n",
              "<P>\n",
              "Animal control officials say they are advising residents in rural areas to be\n",
              "aware of the problem. But so far, no one has been attacked by the wild animals,\n",
              "although there have been such attacks reported elsewhere in Southern\n",
              "California, authorities said.\n",
              "</P>\n",
              "<P>\n",
              "Coyotes have been running amok, officials said. Virtually all parts of the\n",
              "county except beach areas probably have been visited at one time or another by\n",
              "coyotes, said Kathy Jenks, director of the Ventura County Department of Animal\n",
              "Regulation.\n",
              "</P>\n",
              "<P>\n",
              "In Ventura, coyotes are often seen in Grant Park above City Hall, and in Arroyo\n",
              "Verde Park in the Ondulando district on the east side, Jenks said.\n",
              "</P>\n",
              "<P>\n",
              "Elsewhere, coyotes have been seen on streets in Thousand Oaks, Moorpark and\n",
              "Simi Valley. The rural, foothill developments are especially vulnerable, she\n",
              "said.\n",
              "</P>\n",
              "<P>\n",
              "Jenks said she advises residents to keep small pets inside, especially at\n",
              "night.\n",
              "</P>\n",
              "<P>\n",
              "There have even been a few cases in which brazen coyotes have attacked family\n",
              "animals in back yards, and a large number of house cats are disappearing,\n",
              "officials said.\n",
              "</P>\n",
              "<P>\n",
              "\"The common house cat is like a fancy feast for a coyote,\" she said. \"They're\n",
              "hungry, they're thirsty and they're coming down out of the hills.\"\n",
              "</P>\n",
              "<P>\n",
              "There have been a few reports of deer grazing in people's yards, Reese said.\n",
              "</P>\n",
              "<P>\n",
              "Traditionally, September is the worst month for wildlife, authorities said.\n",
              "</P>\n",
              "<P>\n",
              "\"It is usually the driest month,\" Reese said. \"And a lot of animals that have\n",
              "been raised in the spring leave their parents and go in search of food.\"\n",
              "</P>\n",
              "<P>\n",
              "More animals are expected to leave their natural habitats if the drought\n",
              "continues, officials said.\n",
              "</P>\n",
              "<P>\n",
              "\"I tell people who call that if we didn't have the big cats and the coyotes, we\n",
              "would be overrun by rodents,\" Jenks said. \"I would much rather hear a coyote in\n",
              "the distance than have roof rats.\"\n",
              "</P>\n",
              "<P>\n",
              "Don DeBusschere, who lives on a 45-acre walnut orchard in Happy Valley near\n",
              "Ojai, said his family has grown accustomed to wild animals. DeBusschere said he\n",
              "has seen scores of coyotes, several deer and a black bear.\n",
              "</P>\n",
              "<P>\n",
              "Recently, he said, two bobcats have moved into the trees on the edge of his\n",
              "property.\n",
              "</P>\n",
              "<P>\n",
              "\"They're not out to get humans,\" DeBusschere said. \"They're just trying to make\n",
              "a living off the land.\"\n",
              "</P>\n",
              "</TEXT></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RMfMgWt8bgm"
      },
      "source": [
        "Note that the result is exactly the same as displaying the hit contents above. Given the raw text, we can obtain its analyzed form (i.e., tokenized, stemmed, stopwords removed, etc.). Here we show the first ten tokens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgNGM65F6m5h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2665a9f-6823-4754-efcb-a311717c7fcf"
      },
      "source": [
        "analyzed = reader.analyze(doc)\n",
        "analyzed[0:10]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['date',\n",
              " 'p',\n",
              " 'septemb',\n",
              " '27',\n",
              " '1990',\n",
              " 'thursdai',\n",
              " 'ventura',\n",
              " 'counti',\n",
              " 'edit',\n",
              " 'p']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5OUXedN89Yf"
      },
      "source": [
        "The index also stores the raw document vector, which we can obtain as a Python dictionary of analyzed terms to counts (i.e., term frequency).\n",
        "For brevity, we only look at terms that appear more than once:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMtneeJw8HDI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2acbf17a-6592-4f79-9c2d-c86403b8a8d1"
      },
      "source": [
        "doc_vector = reader.get_document_vector('LA092790-0015')\n",
        "{ k: v for (k, v) in doc_vector.items() if v >1 }"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'been': 11,\n",
              " 'habitat': 2,\n",
              " 'year': 3,\n",
              " 'jenk': 4,\n",
              " 'mountain': 3,\n",
              " 'rees': 3,\n",
              " 'would': 2,\n",
              " 'debusscher': 3,\n",
              " 'near': 3,\n",
              " 'hungri': 3,\n",
              " 'past': 2,\n",
              " 'especi': 2,\n",
              " 'wildlif': 2,\n",
              " 'lion': 3,\n",
              " 'ventura': 4,\n",
              " 'anim': 9,\n",
              " 'forest': 2,\n",
              " 'two': 3,\n",
              " 'seen': 3,\n",
              " 'attack': 3,\n",
              " 'black': 2,\n",
              " 'i': 2,\n",
              " 'food': 3,\n",
              " 'counti': 4,\n",
              " 'rural': 3,\n",
              " \"they'r\": 5,\n",
              " 'sever': 3,\n",
              " 'california': 2,\n",
              " 'advis': 2,\n",
              " 'parch': 2,\n",
              " 'area': 4,\n",
              " 'she': 3,\n",
              " 'month': 4,\n",
              " 'deer': 3,\n",
              " 'famili': 2,\n",
              " 'we': 2,\n",
              " 'peopl': 2,\n",
              " 'just': 3,\n",
              " 'live': 2,\n",
              " 'drought': 4,\n",
              " 'ha': 3,\n",
              " 'he': 2,\n",
              " 'hi': 2,\n",
              " 'septemb': 2,\n",
              " 'yard': 2,\n",
              " 'elsewher': 2,\n",
              " 'natur': 2,\n",
              " 'out': 4,\n",
              " 'lot': 2,\n",
              " 'have': 16,\n",
              " 'leav': 2,\n",
              " 'more': 3,\n",
              " 'off': 2,\n",
              " 'ojai': 3,\n",
              " 'report': 4,\n",
              " 'bobcat': 4,\n",
              " 'coyot': 10,\n",
              " 'few': 2,\n",
              " 'bear': 4,\n",
              " 'depart': 2,\n",
              " 'author': 3,\n",
              " 'dry': 2,\n",
              " 'vallei': 4,\n",
              " 'who': 2,\n",
              " 'game': 2,\n",
              " 'resid': 2,\n",
              " 'hous': 3,\n",
              " 'cat': 3,\n",
              " 'sai': 2,\n",
              " 'said': 19,\n",
              " 'park': 2,\n",
              " 'on': 2,\n",
              " 'offici': 5,\n",
              " 'wild': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Ranked Results\n",
        "\n",
        "We can load some standard evaluation sets such as Robust04, which contains 250 queries, or \"topics\" as the TREC conferences call them."
      ],
      "metadata": {
        "id": "GOFVMQKLyW1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyserini.search import get_topics\n",
        "topics = get_topics('robust04')\n",
        "print(f'{len(topics)} queries total')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1sdhbDTdwqf",
        "outputId": "40c188f7-95c0-4d0c-e564-f229b06e7a1c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "250 queries total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The topics are in a dictionary, whose keys are integers uniquely identifying each query. Each topic contains the following fields:\n",
        "\n",
        "* `title`: TREC's term for the brief query a user might actually type;\n",
        "* `description`: a longer form of the query in the form of a complete sentence; and\n",
        "* `narrative`: a description of what the user is looking for and what kinds of results would be relevant or non-relevant."
      ],
      "metadata": {
        "id": "RbODj6sezBvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topics[301]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBOeSkJxy-8R",
        "outputId": "4056086e-3348-4dc4-b409-8b5339bff857"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'narrative': 'A relevant document must as a minimum identify the organization and the type of illegal activity (e.g., Columbian cartel exporting cocaine). Vague references to international drug trade without identification of the organization(s) involved would not be relevant.',\n",
              " 'description': 'Identify organizations that participate in international criminal activity, the activity, and, if possible, collaborating organizations and the countries involved.',\n",
              " 'title': 'International Organized Crime'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the purpose of your experiments, we'll divide them into a development and test set."
      ],
      "metadata": {
        "id": "gMbgbZVqyzdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev_topics = {k:topics[k] for k in list(topics.keys())[:125]}\n",
        "test_topics = {k:topics[k] for k in list(topics.keys())[125:]}"
      ],
      "metadata": {
        "id": "LILkqQDqd3Tj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we'll fetch the relevance judgments for the Robust04 queries, which TREC calls \"qrels\"."
      ],
      "metadata": {
        "id": "HTY9-DMyzuU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "\n",
        "qfile = 'https://github.com/castorini/anserini-tools/blob/63ceeab1dd94c1221f29b931d868e8fab67cc25c/topics-and-qrels/qrels.robust04.txt?raw=true'\n",
        "qrels = []\n",
        "for line in urlopen(qfile):\n",
        "  qid, round, docid, score = line.strip().split()\n",
        "  qrels.append([int(qid), 0, docid.decode('UTF-8'), int(score)])\n",
        "#qrels = [line.strip().split() for line in urlopen(qfile)]"
      ],
      "metadata": {
        "id": "b53vacvvf6fw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each record in the qrel contains four fields:\n",
        "\n",
        "1. the numeric identifier of the query;\n",
        "2. the round of relevance feedback, which is here always 0;\n",
        "3. the identifier of a documennt that has been judged; and\n",
        "4. the relevance score of that document.\n",
        "\n",
        "In Robust04, all relevance judgments are binary, i.e., 1 or 0. Note that not all non-relevant documents are recorded. The qrel file only contains those documents the annotators actually looked at; the vast majority of documents in the collection have not been judged. In IR evaluation, we assume that unannotated documents are non-relevant."
      ],
      "metadata": {
        "id": "BXg8YO590Aky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qrels[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJblOI_pgZBh",
        "outputId": "e8e1d49f-7d0f-4df0-95a6-0a012630dda7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[301, 0, 'FBIS3-10082', 1],\n",
              " [301, 0, 'FBIS3-10169', 0],\n",
              " [301, 0, 'FBIS3-10243', 1],\n",
              " [301, 0, 'FBIS3-10319', 0],\n",
              " [301, 0, 'FBIS3-10397', 1],\n",
              " [301, 0, 'FBIS3-10491', 1],\n",
              " [301, 0, 'FBIS3-10555', 0],\n",
              " [301, 0, 'FBIS3-10622', 1],\n",
              " [301, 0, 'FBIS3-10634', 0],\n",
              " [301, 0, 'FBIS3-10635', 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing Mean Average Precision\n",
        "\n",
        "The Robust04 collection uses binary relevance judgments and usually has multiple relevant results for each query. It is thus common to use **mean average precision** (MAP) to evaluate retrieval performance on it. Remember from class that MAP adds the precision at the position of each _relevant_ document in a ranked list and then divides by the total number of relevant documents. So that we don't have to scan through the entire collection, we usually evaluate MAP at some maximum rank value, such as 100 or 1000. We simply stop scanning at that maximum rank.\n",
        "\n",
        "As we saw above, you should pass a query string (the `title` of a topic) and the desired number of results to the `search` method of the `searcher` object."
      ],
      "metadata": {
        "id": "LoDPnv1b04lP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hits = searcher.search(dev_topics[355]['title'], 1000)\n",
        "[(hit.docid, hit.score) for hit in hits[0:10]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvcF6KPN8jew",
        "outputId": "1a35761d-8a7a-4b36-8bda-2b1edd59079d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('FBIS4-20436', 11.349300384521484),\n",
              " ('FBIS3-23683', 9.126500129699707),\n",
              " ('FBIS3-21238', 8.309300422668457),\n",
              " ('FBIS4-44915', 7.935699939727783),\n",
              " ('FBIS4-20602', 7.6006999015808105),\n",
              " ('FBIS4-47382', 7.529600143432617),\n",
              " ('FT943-1589', 7.480100154876709),\n",
              " ('LA071789-0059', 7.451399803161621),\n",
              " ('FBIS4-22145', 7.215099811553955),\n",
              " ('FBIS4-44667', 7.106500148773193)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this assignment, evaluate MAP@1000 for the list of `test_topics` we created above. You should process the `qrels` data to find the relevant results for each query."
      ],
      "metadata": {
        "id": "407VkQsK8h6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO [35 points]: Compute MAP@1000 for test_topics\n",
        "\n",
        "max_rank = 1000\n",
        "\n",
        "def compute_average_precision(retrieved_docs, relevant_docs):\n",
        "\n",
        "    if not relevant_docs:\n",
        "        return 0.0\n",
        "\n",
        "    num_relevant = 0\n",
        "    precision_sum = 0.0\n",
        "    for i, docid in enumerate(retrieved_docs):\n",
        "        if docid in relevant_docs:\n",
        "            num_relevant += 1\n",
        "            precision_sum += num_relevant / (i + 1)\n",
        "\n",
        "    return precision_sum / len(relevant_docs) if relevant_docs else 0.0\n",
        "\n",
        "def compute_map(test_topics, qrels, retrieval_function, max_rank=1000):\n",
        "\n",
        "    # build a dictionary of relevance judgments\n",
        "    relevance_dict = defaultdict(set)\n",
        "    for qid, _, docid, score in qrels:\n",
        "        if score > 0:\n",
        "            relevance_dict[qid].add(docid)\n",
        "\n",
        "    average_precisions = []\n",
        "\n",
        "    for qid, topic in test_topics.items():\n",
        "        query = topic['title']\n",
        "        retrieved_docs = retrieval_function(query, max_rank)\n",
        "        relevant_docs = relevance_dict[qid]\n",
        "\n",
        "        # calculate average precision\n",
        "        ap = compute_average_precision(retrieved_docs, relevant_docs)\n",
        "        average_precisions.append(ap)\n",
        "\n",
        "    # calculate mean average precision\n",
        "    mean_ap = sum(average_precisions) / len(average_precisions) if average_precisions else 0.0\n",
        "    return mean_ap\n",
        "\n",
        "def bm25_retrieval(query, max_rank=1000):\n",
        "    \"\"\"\n",
        "    Retrieves documents using BM25 model.\n",
        "    :param query: Query string\n",
        "    :param max_rank: Maximum number of documents to retrieve\n",
        "    :return: List of document IDs\n",
        "    \"\"\"\n",
        "    hits = searcher.search(query, max_rank)\n",
        "    return [hit.docid for hit in hits]\n",
        "\n",
        "map_1000_bm25 = compute_map(test_topics, qrels, bm25_retrieval, max_rank)\n",
        "print(f\"MAP@1000 for BM25: {map_1000_bm25:.4f}\")"
      ],
      "metadata": {
        "id": "aGQK-C935EVA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78b1a3e5-38f1-420a-da7b-1cfbb6934748"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAP@1000 for BM25: 0.2616\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing a Query Likelihood Model\n",
        "\n",
        "The default `LuceneSearcher` in pyserini uses a BM25 model. In this part of the assignment, you'll interact directly with the index to implement a query likelihood retrieval model.  For more details on pyserini's index API, [see the documentation](https://github.com/castorini/pyserini/blob/master/docs/usage-indexreader.md).\n",
        "\n",
        "We've already created an `IndexReader` for the Robust04 collection and assigned it to `reader`. Remember that we can pass documents or queries through the index's predefined analysis pipeline, a series of operations like stemming, stopword removal, etc."
      ],
      "metadata": {
        "id": "MpdecWki1sxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reader.analyze('Are there black bear attacks there?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaDprYgo-yRz",
        "outputId": "10bcc3ca-3b7a-4d75-a1f7-b20ff420f6ef"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['black', 'bear', 'attack']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For most retrieval models, including BM25 and query likelihood, we'll want to know the overall statistics for the terms we retrieve from the index. By default, pyserini will do stemming and other \"analysis\" steps on the terms you give it. But since we're already working with stemmed queries, we'll tell it not to perform this analysis now."
      ],
      "metadata": {
        "id": "hs11fpZY-4H8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Just checking that our term is already in its index form.\n",
        "print(reader.analyze('bear'))\n",
        "\n",
        "df, cf = reader.get_term_counts('bear', analyzer=None)\n",
        "print(f'The document frequency is {df} and the collection frequency is {cf}.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNHf3z1Y_gy2",
        "outputId": "d58ab81c-d5d0-497f-fa93-056cbe96490b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['bear']\n",
            "The document frequency is 16429 and the collection frequency is 23545.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might also want to know summary statistics about the collection."
      ],
      "metadata": {
        "id": "FLGz4mncCuyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reader.stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7L0l9hO4Cx5V",
        "outputId": "324b8464-220e-438f-9f2d-19f41f7d4b4b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'total_terms': 174540872,\n",
              " 'documents': 528030,\n",
              " 'non_empty_documents': 528030,\n",
              " 'unique_terms': 923436}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's traverse the postings for a term."
      ],
      "metadata": {
        "id": "o7878QVlAPWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "posting_list = reader.get_postings_list('bear', analyzer=None)\n",
        "print(f'There are {len(posting_list)} postings.')\n",
        "\n",
        "posting = posting_list[100]\n",
        "print(f'Each posting contains a document ID ({posting.docid}), term frequency ({posting.tf}), and position list ({posting.positions}).')\n",
        "\n",
        "print(f'searcher has a method for turning internal integer IDs like {posting.docid} into external IDs like {searcher.doc(posting.docid).docid()}.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWdztSSQAaVs",
        "outputId": "08c9a4c9-33c0-4bcb-af39-c9354d4b9ff3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 16429 postings.\n",
            "Each posting contains a document ID (3405), term frequency (1), and position list ([461]).\n",
            "searcher has a method for turning internal integer IDs like 3405 into external IDs like LA031889-0009.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If, instead of mapping terms to documents, you want to use a document name to get term information, see above for the `get_document_vector` method.\n",
        "\n",
        "Now you should have all the information you need to implement a query likelihood retrieval model. Use Dirichlet smoothing with $\\mu = 550$."
      ],
      "metadata": {
        "id": "F9KdZ6LLMiBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO [50 points]: Implement a query likelihood model with Dirichlet smoothing.\n",
        "## For a given query, index reader, and constant k, return the k documents with the highest query likelihoods.\n",
        "## Think about what data s would let you keep the top k.\n",
        "\n",
        "def query_likelihood_dirichlet(query, reader, mu=550, k=1000):\n",
        "\n",
        "    analyzed_query = reader.analyze(query)\n",
        "\n",
        "    collection_stats = reader.stats()\n",
        "    total_terms = collection_stats['total_terms']\n",
        "\n",
        "    doc_scores = defaultdict(float)\n",
        "    doc_len_cache = {}\n",
        "\n",
        "    for term in analyzed_query:\n",
        "        term_df, term_cf = reader.get_term_counts(term, analyzer=None)\n",
        "        if term_cf == 0:\n",
        "            continue\n",
        "\n",
        "        # get postings\n",
        "        postings = reader.get_postings_list(term, analyzer=None)\n",
        "\n",
        "        for posting in postings:\n",
        "            docid = posting.docid\n",
        "            if docid not in doc_len_cache:\n",
        "                external_docid = searcher.doc(docid).docid()\n",
        "                doc_len = reader.get_document_vector(external_docid)\n",
        "                doc_len_sum = sum(doc_len.values()) if doc_len else 0\n",
        "                doc_len_cache[docid] = (external_docid, doc_len_sum)\n",
        "            else:\n",
        "                external_docid, doc_len_sum = doc_len_cache[docid]\n",
        "\n",
        "            # calculate term score using Dirichlet smoothing\n",
        "            p_ml = posting.tf / doc_len_sum if doc_len_sum > 0 else 0\n",
        "            p_collection = term_cf / total_terms\n",
        "            score = math.log((posting.tf + mu * p_collection) / (doc_len_sum + mu))\n",
        "            doc_scores[external_docid] += score\n",
        "\n",
        "    # sort the documents by score\n",
        "    ranked_docs = sorted(doc_scores.items(), key=lambda item: item[1], reverse=True)[:k]\n",
        "    return [docid for docid, _ in ranked_docs]"
      ],
      "metadata": {
        "id": "PKAzgxzwNQ8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb805a34-34df-4251-86ad-a850a35da354"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 documents for query 'black bear attacks':\n",
            "1. DocID: LA021790-0061\n",
            "2. DocID: LA063090-0144\n",
            "3. DocID: LA102989-0166\n",
            "4. DocID: LA091589-0023\n",
            "5. DocID: LA100489-0043\n",
            "6. DocID: FT942-14032\n",
            "7. DocID: LA052189-0089\n",
            "8. DocID: LA011389-0015\n",
            "9. DocID: LA022589-0035\n",
            "10. DocID: LA051090-0217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can reuse some of your evaluation code for mean average precision from above."
      ],
      "metadata": {
        "id": "xKVt8NkKQlqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO [15 points]: Compute the MAP@1000 for the test set you used above to evaluate BM25.\n",
        "\n",
        "# DISCLAIMER: I could not get this to run in a normal amount of time, I've had it running for 30+ minutes.\n",
        "\n",
        "map_1000_ql = compute_map(test_topics, qrels, lambda q, k: query_likelihood_dirichlet(q, reader, mu=550, k=k), max_rank)\n",
        "print(f\"MAP@1000 for Query Likelihood with Dirichlet smoothing: {map_1000_ql:.4f}\")"
      ],
      "metadata": {
        "id": "67coeQ03NcN6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "outputId": "e1a01130-5f46-43c1-93b9-d289b9530d6a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-00267c664823>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# DISCLAIMER: I could not get this to run in a normal amount of time, I've had it running for 30+ minutes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmap_1000_ql\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqrels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquery_likelihood_dirichlet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m550\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_rank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"MAP@1000 for Query Likelihood with Dirichlet smoothing: {map_1000_ql:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-8b9aca7be46d>\u001b[0m in \u001b[0;36mcompute_map\u001b[0;34m(test_topics, qrels, retrieval_function, max_rank)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mqid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_topics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mretrieved_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrieval_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_rank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0mrelevant_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelevance_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mqid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-00267c664823>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(q, k)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# DISCLAIMER: I could not get this to run in a normal amount of time, I've had it running for 30+ minutes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmap_1000_ql\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqrels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquery_likelihood_dirichlet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m550\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_rank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"MAP@1000 for Query Likelihood with Dirichlet smoothing: {map_1000_ql:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-7f061fc4945b>\u001b[0m in \u001b[0;36mquery_likelihood_dirichlet\u001b[0;34m(query, reader, mu, k)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdocid\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_len_cache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mexternal_docid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mdoc_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_document_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexternal_docid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0mdoc_len_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_len\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdoc_len\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mdoc_len_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdocid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexternal_docid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_len_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyserini/index/lucene/_base.py\u001b[0m in \u001b[0;36mget_document_vector\u001b[0;34m(self, docid)\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0mdoc_vector_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mterm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_vector_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeySet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m             \u001b[0mdoc_vector_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc_vector_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdoc_vector_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}